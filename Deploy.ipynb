{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li9sbP2tv1pS",
        "outputId": "33628162-d3ea-4492-e718-d349c73b4417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkVdpbx7keOy",
        "outputId": "a69f1a00-9f8b-44bc-c6ed-58bf9236329d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_hindi_stopwords(text):\n",
        "\n",
        "  hindi_stopwords = [\n",
        "    'अपना', 'अपनी', 'अपने', 'अभी', 'आदि', 'इस', 'इसके', 'इसको', 'इसमें', 'इससे',\n",
        "    'उनका', 'उनकी', 'उनके', 'उनको', 'उनका', 'उनकी', 'उनके', 'उनको', 'उन्हें', 'उसका',\n",
        "    'उसकी', 'उसके', 'उसको', 'उसमें', 'उसने', 'उसी', 'उसे', 'उसका', 'करता', 'करते',\n",
        "    'करना', 'करने', 'करें', 'कहते', 'कहा', 'का', 'कारण', 'किसी', 'की', 'कुछ', 'के',\n",
        "    'को', 'कोई', 'कौन', 'कौनसा', 'गया', 'घर', 'जब', 'जहाँ', 'जा', 'जितना', 'जिन',\n",
        "    'जिन्होंने', 'जिन्हों', 'जिस', 'जिसे', 'जी', 'जिसके', 'जिसको', 'जिसमें', 'जिसे', 'तक',\n",
        "    'तब', 'तरह', 'तिन', 'तिन्हें', 'तिन्हों', 'तिस', 'तिसे', 'था', 'थे', 'दिया', 'दूसरे',\n",
        "    'दो', 'द्वारा', 'ने', 'पर', 'पहले', 'पूरा', 'पूरी', 'फिर', 'बनी', 'बहुत', 'बाद',\n",
        "    'बाला', 'भी', 'मगर', 'मानो', 'मैं', 'मैंने', 'में', 'यदि', 'यह', 'यहाँ', 'यही',\n",
        "    'या', 'यिह', 'ये', 'रखें', 'रहा', 'रहे', 'लिए', 'लिया', 'लेकिन', 'व', 'वगेरह', 'वर्ग',\n",
        "    'वह', 'वहाँ', 'वही', 'वाले', 'वाली', 'वाला', 'वाले', 'वाली', 'वाला', 'वाले', 'वाली',\n",
        "    'वाला', 'सबसे', 'सबसे', 'सकता', 'सकते', 'सकती', 'सकती', 'सब', 'सबसे', 'से', 'ही',\n",
        "    'है', 'हैं', 'हुआ', 'हुई', 'हुए', 'हो', 'होता', 'होते', 'होती', 'होती', 'होते', 'होते',\n",
        "    'होती', 'होती', 'होती', 'होना', 'होने', 'अंदर', 'अदि', 'अन्य', 'अप', 'अफ', 'अल', 'अलब',\n",
        "    'अव', 'अवश्य', 'अस', 'आदि', 'आप', 'आपका', 'आपकी', 'आपके', 'आपने', 'आपने', 'आदि', 'आना',\n",
        "    'इंहोंने', 'इंहें', 'इंहों', 'इतना', 'इतनी', 'इतने', 'इन', 'इनका', 'इनकी', 'इनके', 'इन्ही',\n",
        "    'इन्होंने', 'इन्हें', 'इन्हों', 'इस', 'इसका', 'इसकी', 'इसके', 'इसी', 'उंहोंने', 'उंहें',\n",
        "    'उंहों', 'उपर', 'उसके', 'उससे', 'उसी', 'उसे', 'उसी', 'उसे', 'कर', 'करत', 'करता', 'करती',\n",
        "    'करते', 'करन', 'करना', 'करने', 'किया', 'किये', 'किये', 'किया', 'कुछ', 'कुल', 'के', 'को', 'को'\n",
        "]\n",
        "\n",
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "  from nltk.corpus import stopwords\n",
        "\n",
        "  import re\n",
        "  nltk.download(\"indian\")\n",
        "  nltk.download(\"stopwords\")\n",
        "\n",
        "  words = nltk.word_tokenize(text)\n",
        "  cleaned_words = []\n",
        "  for word in words:\n",
        "    # Remove specific words\n",
        "    if word.lower() not in hindi_stopwords:\n",
        "      # Remove English words and special characters\n",
        "      if not re.match(\"^[A-Za-z]*$\", word) and not bool(re.search(r'[0-9०-९]', word)):\n",
        "        # Specify special characters to remove (e.g., @, ., ,, #, &)\n",
        "        word = re.sub(r'[.,@#&]', '', word)\n",
        "        cleaned_words.append(word)\n",
        "    return ' '.join(cleaned_words)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QqqqbWBNwXZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define a function for prediction\n",
        "def predict_sentiment(t):\n",
        "    import joblib\n",
        "    # Load the saved pipeline\n",
        "    loaded_pipeline_sentiment = joblib.load('/content/drive/MyDrive/NLP Project/sentiment_classifier.pkl')\n",
        "    loaded_pipeline_news = joblib.load('/content/drive/MyDrive/NLP Project/news_classifier.pkl')\n",
        "\n",
        "    t = remove_hindi_stopwords(t)\n",
        "\n",
        "    sentim = loaded_pipeline_sentiment.predict([t])\n",
        "    auth = loaded_pipeline_news.predict([t])\n",
        "    # if (sentim[0] == 1):\n",
        "    #   sentiment = \"Positive sentiments\"\n",
        "    # if sentim[0] == 0:\n",
        "    #   sentiment = \"Neutral sentiments\"\n",
        "    # if sentim[0] == -1:\n",
        "    #   sentiment = \"Negative sentiments\"\n",
        "    # if auth[0] == 0:\n",
        "    #   auth = 'Fake news'\n",
        "    # if auth[0] == 1:\n",
        "    #   auth = 'True news'\n",
        "\n",
        "    print(f\"Sentiment: {sentim}\")\n",
        "    print(f\"Authenticity: {auth}\")\n",
        "\n",
        "    return (sentim,auth)"
      ],
      "metadata": {
        "id": "1WmovvW2v6Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Enter text\")\n",
        "(sent,auth) = predict_sentiment(inp)"
      ],
      "metadata": {
        "id": "Q289cxrV0r-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95cca410-f5ab-4102-f952-6951705db8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter textहाउस डिम एड: हमने जेसन चैफेट के ट्वीट करने तक कॉमी का पत्र भी नहीं देखा,हाउस डिम एड : हमने अक्टूबर  डारेल ल्यूकस इसे कॉमी पत्र नहीं देखा  अमेरिकन फोर्क  यूटा स्टैम्प जेसन शैफेट्ज सदस्यता लें ( छवि सौजन्य माइकल लॉली  एक क्रिएटिव कॉमन्स-बाय लाइसेंस तहत उपलब्ध ) कीथ ओल्बरमैन माफी साथ  संदेह नहीं कि विश्व खराब व्यक्ति सप्ताह है-एफबीआई निदेशक जेम्स कॉमी। एक हाउस डेमोक्रेटिक सहयोगी अनुसार  ऐसा लगता कि हम जानते कि दूसरा खराब व्यक्ति है। पता चला कि कॉमी अब-कुख्यात पत्र घोषणा कि एफबीआई उन ईमेलों देख जो हिलेरी क्लिंटन ईमेल सर्वर संबंधित  तो संबंधित समितियों रैंकिंग डेमोक्रेट कॉमी बारे सुना नहीं है। रिपब्लिकन कमेटी एक अध्यक्ष एक ट्वीट माध्यम पता चला। जैसा कि अब हम जानते  कॉमी रिपब्लिकन अध्यक्षों और हाउस इंटेलिजेंस  ज्यूडिशियरी  और ओवरसाइट समितियों डेमोक्रेटिक रैंकिंग सदस्यों सूचित कि एजेंसी उन ईमेलों समीक्षा रही थी जिन्हें हाल पता चला कि क्या वे वर्गीकृत जानकारी रखते हैं। पत्र बाहर जाने लंबे समय  ओवरसाइट कमेटी अध्यक्ष जेसन चैफेट्ज़ ट्वीट साथ राजनीतिक दुनिया आग लगा दी। एफबीआई डर मुझे सूचित  `` एफबीआई उन ईमेलों अस्तित्व बारे सीखा जो जांच अनुसार प्रतीत हैं। '' मामला खुल - जेसन शैफेट्ज़ (  ) अक्टूबर  बेशक  अब हम जानते कि मामला नहीं था। कॉमी वास्तव कह कि `` एक असंबंधित मामला '' प्रकाश ईमेल समीक्षा  जो अब हम एक किशोर साथ एंथोनी वेनर सेक्सटिंग जानते हैं। स्पष्ट रूप छोटी चीजें तथ्य रूप महत्वपूर्ण नहीं थीं। यूटा रिपब्लिकन जांच शुरू कसम खाई थी कि अगर हिलेरी कम कम साल जीतती  और संभवत : पूरे कार्यकाल लायक हैं। जाहिर तौर शैफेट्ज लगा कि एफबीआई काम रही - परिणामस्वरूप एक ट्वीट देश हल्का  कि कूलर प्रमुखों एहसास कि एक कठिन घटना है। एक वरिष्ठ हाउस डेमोक्रेटिक सहयोगी अनुसार  उस पत्र गलत तरीके फैलाना शायद चैफेट पापों कम कम हो। उस सहयोगी शेयरब्लू बताया कि बॉस और डेमोक्रेट उस समय कॉमी पत्र बारे नहीं पता - और केवल तभी पता चला उन्होंने ट्विटर जाँच की। `` संबंधित समितियों डेमोक्रेटिक रैंकिंग सदस्यों रिपब्लिकन अध्यक्षों कॉमी पत्र नहीं मिला। वास्तव डेमोक्रेटिक रैंकिंग सदस्यों इसे प्राप्त नहीं कि ओवरसाइट और सरकारी सुधार समिति अध्यक्ष  जेसन चैफेट्ज़ इसे ट्वीट नहीं और इसे सार्वजनिक दिया। ” तो चलो देखते कि क्या हमें अधिकार मिला है। एफबीआई निदेशक शैफेट और जीओपी समिति अध्यक्षों संभावित रूप राजनीतिक रूप विस्फोटक जांच एक प्रमुख विकास बारे बताता  और न शैफेट और सहयोगियों डेमोक्रेटिक समकक्षों बारे बताने शिष्टाचार था। बजाय  सहयोगी अनुसार  ट्विटर बारे पता लगाया। डेली कोस बात चुकी कि कॉमी खुद शैफेट और रिपब्लिकन पत्र अग्रिम नोटिस  जिससे स्पिन मशीन चालू समय मिल गया। कि अच्छे थिएटर बना  ऐसा नहीं जो बताता कि मामला है। आखिरकार  ऐसा नहीं जो बताता कि कॉमी घोर अक्षम और टोन-बहरा अलावा नहीं था। हालांकि  सुझाव देता कि चैफेट्ज़ एक अभिनय  जो डैन बर्टन और डेरेल इस्सा ज़िम्मेदारी और द्विदलीयता मॉडल दिखता है। पास विस्फोटक बारे रैंकिंग सदस्य एलिजा कमिंग्स सूचित शालीनता नहीं थी। निष्पक्षता बुनियादी मानकों नहीं रौंदता  तो मुझे नहीं पता कि क्या है। दी गई  संभावना नहीं कि शैफेट जवाब देना होगा। प्रोवो और ओरेम लंगर डाले एक हास्यास्पद रिपब्लिकन जिले बैठता ; + कुक पार्टिसन वोटिंग इंडेक्स  और मिट रोमनी प्रतिशत वोट दंड था। अलावा  रिपब्लिकन हाउस नेतृत्व शैफेट योजनाबद्ध मछली पकड़ने अभियान पूर्ण समर्थन है। मतलब नहीं कि हम उस गर्म रोशनी नहीं डाल सकते। आखिरकार  एक पाठ्यपुस्तक उदाहरण जो सदन रिपब्लिकन नियंत्रण है। और दुनिया दूसरा बुरा व्यक्ति है। डेरेल लुकस बारे डारेल उत्तरी कैरोलिना विश्वविद्यालय स्नातक जो खुद पुराने स्कूल पत्रकार मानता है। कॉलेज धार्मिक अधिकार एक सदस्य रूप बदलने प्रयास केवल धार्मिक अधिकार बुरे सपने बदलने सफल - एक करिश्माई ईसाई जो एक अप्रकाशित उदारवादी है। उन लोगों खड़े इच्छा जो केवल चुप्पी डर गए  एक अपमानजनक तीन साल शादी बच गए। ईपू  डेली कोस ईसाई मांग रूप जान हैं। ट्विटर  अनुसरण फेसबुक साथ जुड़ें। खरीदने यहां क्लिक करें। जुडिये\n",
            "Sentiment: [1]\n",
            "Authenticity: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]   Package indian is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent,auth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_roxL42Qk6pt",
        "outputId": "02efa2ed-274f-4cd2-c9c9-4084bb59c198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Positive sentiments', 'True news')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OeX7BkZMlthU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}